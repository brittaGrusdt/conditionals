// State prior  --------------------------------------------------------------
// var p_ac_ind = {"none": 0.25,
//                 "lawn": 0.0,
//                 "pizza" : 1,
//                 "dutchman" : 1
//                 }
// var cn_prior = function(bias) {
//   var p_ind = p_ac_ind[bias]
//   var p_dep = (1 - p_ind) / (globalStore.cns.length - 1)
//   var ps = map(function(cn){cn == "A || C" ?  p_ind : p_dep}, globalStore.cns)
//   return categorical({vs: globalStore.cns, ps: ps})
// }
var log_likelihood = function(state){
  var p = cn_to_prob(state)
  return state.cn == "A || C" ?
  //todo: should be truncated normal here!! (between 0 and min(pa=p[0], pc=p[1]))
    Gaussian({mu:p[0]*p[1], sigma: globalStore.indep_sigma}).score(p[2]) :
    //Math.log(-Math.log(p[0]*p[1])) :
    // P(A) and P(C) supposed to be uniform distributed over [0,1] hence
    // their product follows -log(pa x pc)
    state.cn == "A,B->D->C" ?
      ((p[0] != 0 || p[3] != 0) ? -Infinity :
    (Beta({a:10, b:1}).score(p[1]) + Beta({a:10, b:1}).score(p[2]))) :

    state.cn == "A,B->C" ?
      (p[3] != 0 ? -Infinity : (Beta({a:10, b:1}).score(p[0]) +
      Beta({a:10, b:1}).score(p[1]) + Beta({a:10, b:1}).score(p[2]))) :

    state.cn == "only A implies C" ?
      (p[1] != 0 ? -Infinity : Beta({a:10, b:1}).score(p[0])) :

    // for all usual dependent cns
    (Beta({a:10, b:1}).score(p[0]) + Beta({a:1, b:10}).score(p[1]))
}

var log_likelihood_dirichlet = cache(function(state){
  // display('in loglikelihood dirichlet')
  var par = filter(function(obj){
    return(obj.id == state.cn)
  }, globalStore.params_ll)[0]
  var alphas = [par["alpha_1"], par["alpha_2"], par["alpha_3"], par["alpha_4"]]
  // display(state)
  var ll = Dirichlet({"alpha": Vector(alphas)}).score(Vector(state.table.params.ps))
  return(ll)
})


var state_prior = cache(function(bias) {
  var distr = bias == "uniform-prior-states" ?
  // all input tables are equally likely (no-causal nets)
    Infer({method: 'enumerate', model:function(){
      var TableID = uniformDraw(globalStore.Tables)
      var Table = TableID.Table
      var intention = uniformDraw(globalStore.speaker_intents)
      var state = {"table": Table, "cn": "no-cn", "id": TableID.id};
      return {"bn": state, "intention": intention}
    }}) :
    Infer({method:'enumerate', model:function(){
      var TableID = uniformDraw(globalStore.Tables)
      var Table = TableID.Table
      var cn = uniformDraw(globalStore.cns);
      var state = {"table": Table,
                   "cn": cn,
                   "id": TableID.id};
      if(bias=='lawn'){
      } else if(bias=="pizza"){
        condition(state.cn == "A || C")
      } else if(bias == "dutchman"){
        condition(marginal(Table, ["C"]) <= thresholds.f)
      }
      let logl =
        state.cn=="A implies C" ? TableID.logL_if_ac :
        state.cn=="A implies -C" ? TableID.logL_if_anc :
        state.cn=="C implies A" ? TableID.logL_if_ca :
        state.cn=="C implies -A" ? TableID.logL_if_cna :
        state.cn=="A || C" ? TableID.logL_ind :
        // log_likelihood(state);
        log_likelihood_dirichlet(state);

      let log_val = !logl ? -Infinity : logl;
      factor(log_val);
      var intention = uniformDraw(globalStore.speaker_intents)
      return {"bn": state, "intention": intention}
    }});
  // make sure that states that have almost 0-probability are excldued,
  // otherwise these states face a problem for the speaker who cannot say
  // anything because the log of the literal listener will always be -Infinity
  return Infer({model:function(){
    var s = sample(distr)
    condition(Math.exp(distr.score(s)) > 0.0000011)
    return s
  }})
})
