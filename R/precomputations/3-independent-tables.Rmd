---
title: "Likellihood independent tables"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tibble)
library(purrr)
```

**Step 1**

This is done in script *1-wiggled-ind-tables.R*.

Generate a set of independent tables (1000) and wiggle each table w (500) times.
Each table entry is wiggled by sampling a value from a beta distribution with 
the following parameters:

* $\alpha=cell * noise_v$
* $\beta=(1-cell) * noise_v$

The greater the parameter noise_v is, the tighter the beta distribution is
around the means, i.e. the table entries.


**Example**

One particular table and particular parameter noise_v

```{r}
table <- c(0.4, 0.3, 0.1, 0.2)
table <- tibble(AC=0.4, `A-C`=0.3, `-AC`=0.1, `-A-C`=0.2) %>%
            gather(`AC`, `A-C`,`-AC`, `-A-C`, key=cell, value=p)
noise_v <- 250
x <- table$p %>% map(function(p){
rbeta(10000, p*noise_v, (1-p)*noise_v) %>% as_tibble()})  %>% bind_rows(.id = "cell")

pw <- x %>% ggplot(aes(x=value, color=cell)) +
                    geom_density() +
                    facet_wrap(~cell) + 
                    labs(title =  paste("noise_v=", noise_v))

pw
```


**Step 2: Log likelihood**

This is done in script *2-approx-likelihood.R*.

Find a value that can be computed for each table, such that it correlates
well with the log likelihood of the respective table.

The log likelihood to that a table was generated by a independent network is the
log likelihood to generate a particular table (being constant for all tables, 
since P(A) and P(C), which are used to compute the table, come from a uniform
distribution) times plus the log likelihood of the wiggled cell entries coming
from the respective beta distributions.


**Step 2: Computed values**

The following values tend to be small, for tables where A and C are independent:

* x1: P(A,C) - P(A) * P(C)
* x2: P(C|A) - P(C|-A)
* x3: P(C|A) - P(C)

The smaller these values are, the more the table should be believed to come from
an independent network.

Read the computed data

```{r}
measurements <- readRDS("../../data/precomputations/wiggled-tibbles-measurements.rds")
noise_params <- measurements$noise_v
```

Plot distribution of these values

* x1: P(A,C) - P(A) * P(C)
* x2: P(C|A) - P(C|-A)
* x3: P(C|A) - P(C)


```{r}
for(i in seq(1, nrow(measurements))){
  data <- measurements[i,]$table_measurements[[1]]
  p2 <- data %>%
          ggplot() +
            geom_density(aes(x=abs(strength), color=measurement)) +
            facet_wrap(~measurement, scales="free_y") +
            ggtitle(paste("noise_v:", noise_params[[i]]))  + 
            xlim(0, 0.5)
  print(p2)
  
  # p1 <- data %>% 
  #       ggplot() +
  #         geom_density(aes(x=abs(strength),color=measurement)) + 
  #         facet_wrap(~measurement, scales="free_y") +
  #         ggtitle(paste("noise_v:", noise_params[[i]]))
  # 
  # print(p1)
  # 
}
```

**Step 3: Correlation log-likelihood and computed values**

Plot relation between log likelihood and measurements and find the value
(x1,x2 or x3) with that correlates best with the log likelihood.

Pearson's correlation coefficient is in a range from -1 to 1; the larger the
distance to one, the larger the correlation is; a positive value indicates a 
positive dependency while negative values mean that when one variable increases,
the other variable decreases.

```{r}
corrs <- list()
for(i in seq(1, nrow(measurements))){
  v <- noise_params[[i]]
  
  new_tables <- measurements[i,]$table_measurements[[1]]
  metrics <- new_tables %>% 
              select(measurement, strength, logLik, table_id, wiggle_id) %>% 
              mutate(strength_abs=abs(strength)) 
  
  # p1 <- new_tables  %>% 
  #       ggplot(mapping=aes(x=strength, y=logLik)) + 
  #       geom_bin2d(bins = 50) +
  #       facet_wrap(~measurement, scales="free") +
  #       theme(axis.text.x = element_text(angle=90)) +
  #       ggtitle(paste("noise param v:", v))
  # 
  # 
  # print(p1)
  
  p2 <- metrics  %>%
        ggplot(aes(x=strength_abs, y=logLik)) +
        geom_bin2d(bins=50) +
        scale_x_continuous(trans='log2') +
        facet_wrap(~measurement, scales="free") +
        theme(axis.text.x = element_text(angle=90)) +
        ggtitle(paste("noise param v:", v))
  print(p2)
  
  # Correlation coefficients
  groups <- group_by(metrics, measurement)
  correlations <- summarize(groups, corr=cor(logLik, strength_abs))
  c <- correlations %>% add_column(noise_v=v)
  corrs[[i]] <- c
  
} 
corrs <- bind_rows(corrs)
corrs
```

**Step 4 - approximate distribution of the value yielding best correlation**

This is also done in script *2-approx-likelihood.R*.

We optimize the parameters of a beta distribution to fit the distribution
of the absolute values of the observed values of x1, i.e. for
$|P(A)\cdot P(C) - P(A,C)|$.


**Step 5 - how good is the model?**

**Simulation of p-values**

Load optimized paramters of beta distribution used to approximate log-likelihood.

```{r}
path_data <- "/home/britta/UNI/Osnabrueck/MA-project/conditionals/data/precomputations/best-params.rds"
params <- readRDS(path_data)
params
```


reminder: p-value is always dependent on the observed data!
Goal: we want to measure whether our model is a good model for our data.

```{r, echo=FALSE}
alpha <- params$alpha_hat
beta <- params$beta_hat

data <- measurements %>% filter(noise_v==params$noise_v) %>%
  pull(table_measurements)
data <- data[[1]] %>% filter(measurement == params$value_m)

n <- data %>% nrow()

# log likelihood of data under model
ll_y <- sum(dbeta(abs(data$strength), alpha, beta))
```

We repeatedly sample n data points from the assumed underlying model.

* The computed measurement values (from the wiggled tables) can be considered as
being the observed data collected in an experiment. 

* Now, we simulate repetitions of the "experiment" by repeatedly sampling the
same number of data points from the assumed distribution (i.e. a
beta-distribution with the optimized parameters) and each time we compute the
probability of the sampled data under the model.

* If the model is a good model for our data, the histogram of the differences of
these likelihoods and the likelihood of the data given the model should contain
0 and the likelihood of the actual data under the model should be in accordance
with the distribution of the likelihoods of the repeated data.

* We can also check each time whether the likelihood of the repeated data is
greater or equal to the likelihood of the actual data under the model.
That is, we may simulate how likely it is to observe a value greater or equal
to the actually observed value. If that value is close to 0 or close to 1, 
the model does not  seem to be adequate for our data.

```{r}
ll_y_rep <- readRDS("../../data/precomputations/ll-y_rep.rds")
p <- ggplot(ll_y_rep) + geom_density(aes(x=ll_y_rep))
p + geom_vline(xintercept = ll_y, color="green")


diffs <- as.double(ll_y_rep$ll_y_rep) - ll_y
hist(diffs)
mean(ll_y_rep$ll_y_rep >= ll_y)
```


Plot the distribution of the log likelihoods for each causal network.

```{r}
logLiks <- readRDS("../../data/precomputations/logLik.rds")

evs <- logLiks %>% group_by(cn) %>% summarize(ev=mean(logL))

logLiks %>% ggplot(aes(x=logL, color=cn)) +
                    geom_density() +
                    facet_wrap(~cn) + 
                    geom_vline(aes(xintercept=ev), data = evs)
```


The expected value of the log likelihood for the independent tables is much greater than it is for the dependent networks.

```{r}
ll_ind <- evs %>% filter(cn=="A || C") %>% pull(ev)
ev <- evs %>% pull(ev)

evs <- evs %>% mutate(greater_ind=ev/ll_ind)
evs
evs %>% filter(cn != "A || C") %>% summarize(mean=mean(greater_ind))
```


