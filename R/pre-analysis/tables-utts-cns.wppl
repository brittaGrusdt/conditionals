////////////  SET PARAMETERS  /////////////////
globalStore.thetaCP = 0.05
var noise_independent = data["noise"][0]
var n_tables = data["n_tables"][0]
var verbose = data["verbose"][0]
var bias = data["bias"][0]

var thresholds = {t: 0.899, f: 0.0499, theta: 0.899, theta_maybe : 0.499}

var vars = [["A", "-A"], ["C", "-C"]]

// Helper --------------------------------------------------------------------
var negate = function(x){
  return x.startsWith("-") ? x.slice(1) : "-" + x
}

var combinations = Infer({model:function(){
  var bools = repeat(vars.length, flip)
  var tokens = mapIndexed(function(idx, b){
    b ? uniformDraw(vars[idx]) : ""
  }, bools)
  return filter(function(t){t.length>0}, tokens)
}}).support()
var combinations = filter(function(x){x.length > 0}, combinations)

var combine_pairs = function(connective){
  var pairs_list = mapIndexed(function(idx, tokens1){
    var arr = filter(function(tokens){
      all(function(x){
        !tokens.includes(x) && !tokens.includes(negate(x))
      }, tokens1)
    }, combinations.slice(idx+1))

    var antecedent = tokens1.join(" and ")
    var ifs = map(function(tokens2){
      antecedent + connective + tokens2.join(" and ")
    }, arr)

    var ifs_rev = map(function(tokens2){
      tokens2.join(" and ") + connective + antecedent
    }, arr)
    return ifs.concat(ifs_rev)
  }, combinations)

  var pairs = reduce(function(arr, acc){
    acc.concat(arr)
  }, [], pairs_list)
  return pairs
}

var intersect_arrays = function(arrays){
  return filter(function(m){
          var m_in_all_lists = map(function(idx){arrays[idx].includes(m)},
                                  _.range(1,arrays.length))
          return sum(m_in_all_lists)==m_in_all_lists.length
    }, arrays[0])
}

// Causal networks  ----------------------------------------------------------
var dependent_nets = combine_pairs(" implies ")
var causal_nets = ["A || C"].concat(dependent_nets)

// Probabilities ------------------------------------------------------------
var marginal = cache(function(table, variables){
  // computes the probability of P(A) marginalized over all other variables in
  // support of table with P=table and variables=["A"]
  var tokens = table.support()
  var all_x = map(function(v){
    v.includes("-") ? filter(function(k){k.includes(v)}, tokens) :
                      filter(function(k){!k.includes("-"+v)}, tokens)
  }, variables)
  var xs = intersect_arrays(all_x)

  return reduce(function(x, acc){acc + Math.exp(table.score(x))}, 0, xs)
})

var p_ac_ind = {"none": 1/2,
                   "lawn": 0.1,
                   "pizza" : 1,
                   "BC-impossible-cons": 1
                  }

var cn_to_prob = cache(function(state){
  var cn = state.cn
  var table = state.table
  cn == "A implies C" ? marginal(table, ["A", "C"]) / marginal(table, ["A"]) :
  cn == "A implies -C" ? marginal(table, ["A", "-C"]) / marginal(table, ["A"]) :
  cn == "-A implies C" ? marginal(table, ["-A", "C"]) / marginal(table, ["-A"]):
  cn == "-A implies -C" ? marginal(table, ["-A", "-C"])/marginal(table, ["-A"]):
  cn == "C implies A" ? marginal(table, ["A", "C"]) / marginal(table, ["C"]) :
  cn == "C implies -A" ? marginal(table, ["-A", "C"]) / marginal(table, ["C"]) :
  cn == "-C implies A" ? marginal(table, ["A", "-C"]) / marginal(table, ["-C"]):
  cn == "-C implies -A" ? marginal(table, ["-A", "-C"])/marginal(table, ["-C"]):
  cn == "A || C" ? Math.abs(marginal(table, ["A", "C"]) -
                            (marginal(table, ["A"]) * marginal(table, ["C"]))
                           ) :
  error('unknown network: ' + cn)
}, 10000)

// States --------------------------------------------------------------------
var build_table_functions = {
  "A/-A implies" : cache(function(c_a, c_na, a){
                          return {"AC": c_a * a, "A-C": (1-c_a) * a,
                                  "-AC": c_na * (1-a), "-A-C": (1-c_na) * (1-a)
                                 }
                         }),
  "C/-C implies" : cache(function(a_c, a_nc, c){
                          return {"AC": a_c * c, "A-C": a_nc * (1-c),
                                  "-AC": (1-a_c) * c, "-A-C": (1-a_nc) * (1-c)
                                 }
                         }),

  "||" : cache(function(a, c){
                var epsilon = uniform({a:0, b:noise_independent})
                var table = map(function(t){t + epsilon},
                                [a*c, a * (1-c), (1-a) * c, (1-a) * (1-c)])
                var c = sum(table)

                return {"AC": table[0] / c, "A-C": table[1] / c,
                        "-AC": table[2] / c, "-A-C": table[3] / c}
                })
}

var make_tables = cache(function(cn){
  var build_table = cn.includes(" || ") ? build_table_functions["||"] :
                      cn.startsWith("A") || cn.startsWith("-A") ?
                        build_table_functions["A/-A implies"] :
                        build_table_functions["C/-C implies"]

  var table_distributions =
    Infer({method:'forward', samples:n_tables, model:function(){
      var child_p = beta(10,1)
      var child_np = beta(1,10)
      var parent = beta(1,1)
      var probs =
        cn=="A || C" ? [beta(1,1), beta(1,1)] :
        cn=="A implies C" || cn=="C implies A" ? [child_p, child_np, parent] :
        cn=="A implies -C" || cn=="C implies -A" ?
          [1-child_p, 1-child_np, parent] :
        cn=="-A implies C" || cn=="-C implies A" ?
          [child_np, child_p, 1-parent] :
        cn=="-A implies -C" || cn=="-C implies -A" ?
          [1-child_np, 1-child_p, 1-parent] :
        error("unknown causal network " + cn)

      var table = cn=="A || C" ? build_table(probs[0], probs[1])
                                 : build_table(probs[0], probs[1], probs[2])
      return Categorical({"vs": Object.keys(table),"ps": Object.values(table)})
  }}).support()
  return table_distributions
}, 10000)

var tables = reduce(function(cn, acc){
  acc.concat(make_tables(cn))
}, [], causal_nets)
display('# tables: ' + tables.length)

var tables_return = map(function(cn){
  var tables_cn = make_tables(cn)
  map(function(t){t["params"]["ps"].concat(cn)}, tables_cn)
}, causal_nets)

var cn_prior = function(bias) {
  var p_ind = p_ac_ind[bias]
  var p_dep = (1 - p_ind) / dependent_nets.length
  var ps = map(function(cn){cn=="A || C" ?  p_ind : p_dep}, causal_nets)
  return categorical({vs: causal_nets, ps: ps})
}

var log_likelihood = function(state){
  var p = cn_to_prob(state)
  return state.cn == "A || C" ?  -Math.log(p) : -Math.log(1-p)
  // Math.log(p) : Beta({a:10, b:1}).score(p)
}

var state_prior = cache(function(bias){
  return Infer({method:'enumerate', model:function(){
    var state = {"table": uniformDraw(tables),
                 "cn": cn_prior(bias)}
    // var association_strength = log_likelihood(state)
    // factor(association_strength)

    if(bias == 'lawn'){
      if(Math.exp(state.table.score("-AC")) <= globalStore.thetaCP){
        factor(-Math.log(thresholds.f))
      }
    }else if(bias == "BC-impossible-cons"){
      condition(marginal(state.table, ["C"]) <= thresholds.f)
    }
    return state
  }})
})

var all_states = state_prior(bias).support()
if(verbose){ display('# states: ' + all_states.length)}

// Utterances and meaning ----------------------------------------------------
var literals_conjunctions = map(function(tokens){
  tokens.join(" and ")
}, combinations)
if(verbose){
  display('# literals + conjunctions: ' + literals_conjunctions.length)
}

var literals = filter(function(utt){!utt.includes(" and ")},
                      literals_conjunctions)
var maybes = map(function(utt){'maybe ' + utt}, literals)
if(verbose){ display('# maybe: ' + maybes.length) }

var conditionals = combine_pairs(" > ")
if(verbose){ display('# conditionals: ' + conditionals.length) }

var utterances = literals_conjunctions.concat(conditionals.concat(maybes))
if(verbose){
  display('' + '# all utterances: ' + utterances.length)
}

var utterance_probs = cache(function(utterance, table){
  if(conditionals.includes(utterance)){
    var components = utterance.split(" > ")
    var antecedent = components[0].split(" and ").join("")
    var consequent = components[1].split(" and ").join("")
    return marginal(table, [antecedent, consequent]) /
           marginal(table, [antecedent])
  }
  if(literals.includes(utterance)){return marginal(table, [utterance])}
  if(maybes.includes(utterance)){
    var u = utterance.slice("maybe ".length)
    return marginal(table, [u])
  }
  if(literals_conjunctions.includes(utterance)){
    var components = utterance.split(" and ")
    return marginal(table, components)
  }
  else{error("unknown utterance " + utterance)}
}, 10000)

var meaning = cache(function(utterance, state){
 var p = utterance_probs(utterance, state.table)
 var u_applicable = utterance.includes('maybe') ?
     (p >= thresholds.theta_maybe) : p >= thresholds.theta
 return u_applicable
})

var utts_to_remove = filter(function(u){
  !any(function(s){meaning(u, s)}, all_states)
}, utterances)

if(verbose){
  display("")
  display('# utts without corresponding state: ' + utts_to_remove.length)
  map(display, utts_to_remove)
}

var utterances = filter(function(u){!utts_to_remove.includes(u)}, utterances)
if(verbose){
  display("")
  display('# included utterances: ' + utterances.length)
  map(display, utterances)
}

var obj = {
  "utterances" : utterances,
  "tables" : tables_return,
  "cns" : causal_nets
}
obj

// Get prior distribution
// var prior = state_prior("none")
// display('# states: ')
// display(prior.support().length)
//
// var obj = {"prior": prior} //, "tables": tables_return}
// obj
